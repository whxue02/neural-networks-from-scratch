{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb7d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read the CSV file \"clean_weather.csv\" into a DataFrame.\n",
    "# set the first column (usually the date or unnamed index) as the DataFrame index.\n",
    "data = pd.read_csv(\"clean_weather.csv\", index_col=0)\n",
    "\n",
    "# forward-fill missing values (NaNs) in the DataFrame using the last known non-missing value.\n",
    "# this is useful in time series data where missing values can be inferred from prior entries.\n",
    "data = data.ffill()\n",
    "\n",
    "# create a scatter plot to visualize the relationship between today's maximum temperature (tmax)\n",
    "# and tomorrow's maximum temperature (tmax_tomorrow). This helps assess if a linear trend exists.\n",
    "data.plot.scatter(\"tmax\", \"tmax_tomorrow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see correlation coefficient (how linear)\n",
    "data.corr()\n",
    "\n",
    "# 0.81 coefficient (strong, but not perfect linear relation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da967848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a scatter plot using the DataFrame 'data'\n",
    "# plot today's max temperature (\"tmax\") on the x-axis\n",
    "# and tomorrow's max temperature (\"tmax_tomorrow\") on the y-axis\n",
    "data.plot.scatter(\"tmax\", \"tmax_tomorrow\")\n",
    "\n",
    "# define a simple linear prediction function using a lambda\n",
    "# it predicts tomorrow's temperature based on today's temperature (x)\n",
    "# using a slope (w1) of 0.82 and a y-intercept (b) of 11.99\n",
    "prediction = lambda x, w1=.82, b=11.99: x * w1 + b\n",
    "\n",
    "# plot the prediction line in green from x = 30 to x = 120\n",
    "# this shows the linear model over the scatter plot\n",
    "plt.plot([30, 120], [prediction(30), prediction(120)], 'green')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928e9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define a function to calculate mean squared error\n",
    "def mse(actual, predicted):\n",
    "    # subtract the predicted values from the actual values, square the differences,\n",
    "    # then take the mean of all squared differences\n",
    "    return np.mean((actual - predicted) ** 2)\n",
    "\n",
    "# calculate and print the mean squared error between the true maximum temperatures \n",
    "# for tomorrow and the predicted values based on today's temperatures\n",
    "print(mse(data[\"tmax_tomorrow\"], prediction(data[\"tmax\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee92ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the \"tmax\" column into 25 equally sized temperature intervals (bins)\n",
    "tmax_bins = pd.cut(data[\"tmax\"], 25)\n",
    "\n",
    "# compute a ratio for each row: how much tomorrow's max temperature deviates from 11.99,\n",
    "# scaled by today's temperature\n",
    "ratios = (data[\"tmax_tomorrow\"] - 11.99) / data[\"tmax\"]\n",
    "\n",
    "# for each bin of \"tmax\", calculate the average of the corresponding ratios\n",
    "binned_ratio = ratios.groupby(tmax_bins).mean()\n",
    "\n",
    "# for each bin, also calculate the average value of \"tmax\" itself\n",
    "binned_tmax = data[\"tmax\"].groupby(tmax_bins).mean()\n",
    "\n",
    "# plot the average \"tmax\" (x-axis) against the average ratio (y-axis) per bin\n",
    "plt.scatter(binned_tmax, binned_ratio)\n",
    "\n",
    "# tomorrow’s temperature tends to be lower than today’s temperature plus a fixed offset (around 12)\n",
    "# but around 120, the temp is like 30% less plus 12\n",
    "# example of a nonlienar regressio that linear regression cant model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f1b6a",
   "metadata": {},
   "source": [
    "neural networks do three key things\n",
    "\n",
    "- add a non-linear transformation on top of linear transformation\n",
    "- ultple layers, which can capture interactions between features\n",
    "- multiple hidden units, which can have slightly different linear and nonlinear transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fdc77b",
   "metadata": {},
   "source": [
    "ACTIVATION FUNCTION\n",
    "- most common is ReLU (set everything below 0 to 0)\n",
    "- EXAMPLE:\n",
    "$y = wx + b$\n",
    "into\n",
    "$y = relu(wx+b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage of the ReLU activation function\n",
    "temps = np.arange(-50,50)\n",
    "plt.plot(temps, np.maximum(0, prediction(temps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84095d84",
   "metadata": {},
   "source": [
    "MULTIPLE LAYERS\n",
    "- two layer neural network\n",
    "- instead of just $\\hat{y} = relu(wx+b)$, we have $\\hat{y} = w_{2}relu(w_{1}x + b_{1}) + b_{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31921ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage of the ReLU activation function and multiple layers\n",
    "temps = np.arange(-50,50)\n",
    "layer1 = np.maximum(0, prediction(temps))\n",
    "# supposed to be learned weights and biases\n",
    "layer2 = prediction(layer1, 0.5, 10)\n",
    "\n",
    "# plot\n",
    "plt.plot(temps, layer2)\n",
    "plt.ylim((0,40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6414cfab",
   "metadata": {},
   "source": [
    "MULTIPLE NODES\n",
    "- more nodes per layer help create nonlinear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eaa62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example usage of the ReLU, multiple layers, and multiple nodes\n",
    "layer1_1 = np.maximum(0, prediction(temps))\n",
    "# nn will calc weight and bias\n",
    "layer1_2 = np.maximum(0, prediction(temps, .1, 10))\n",
    "layer1_3 = np.maximum(0, prediction(temps, 2, -50))\n",
    "\n",
    "# add layer 2 with new weights and bias\n",
    "layer2 = layer1_1 * 0.5 + layer1_2 * 0.3 + layer1_3 * 0.4 + 20\n",
    "\n",
    "plt.plot(temps, layer2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27453628",
   "metadata": {},
   "source": [
    "how do we keep track of everything?\n",
    "- using matricies!\n",
    "- Inputs $$\\begin{bmatrix} 80 \\\\ 90 \\end{bmatrix}$$ \n",
    "- layer 1 weights $$\\begin{bmatrix} 0.82 & 0.1 \\end{bmatrix}$$\n",
    "- layer 1 biases $$\\begin{bmatrix} 11.99 & 10 \\end{bmatrix}$$\n",
    "- multiplying input and weights matricies, then adding biases gives the layer 1 output: $$\\begin{bmatrix} 77.59 & 18 \\\\ 85.79 & 19 \\end{bmatrix}$$\n",
    "- more input = more rows to output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02601b74",
   "metadata": {},
   "source": [
    "FORWARD PASS EXAMPLE (with made up weights and bias)\n",
    "- used when we know weights and bias, so that a prediction can be made\n",
    "- replace single points with matrices\n",
    "- new equation is $layer_{1} = relu(XW_{1} + B_{1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6858eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the math behind it\n",
    "import tsensor\n",
    "\n",
    "input = np.array([[80], [90], [100], [-20], [-10]])\n",
    "\n",
    "l1_weights = np.array([[.82, .1]])\n",
    "l1_bias = np.array([[11.99,10]])\n",
    "with tsensor.explain():\n",
    "    l1_output = input @ l1_weights + l1_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be6e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce979fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu activation\n",
    "l1_activated = np.maximum(0, l1_output)\n",
    "l1_activated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bfe6bc",
   "metadata": {},
   "source": [
    "apply layer 2\n",
    "- $\\hat{y} = W_{2}relu(XW_{1} + B_{1}) + B_{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61942bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_weights = np.array([[0.5], [0.2]])\n",
    "l2_bias = np.array([5])\n",
    "\n",
    "with tsensor.explain():\n",
    "    output = l1_activated @ l2_weights + l2_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for gradient descent example\n",
    "tmax = np.array([[80], [90], [100], [-20], [-10]])\n",
    "tmax_tomorrow = np.array([[83], [89], [95], [-22], [-9]])\n",
    "\n",
    "def mse(actual, predicted):\n",
    "    return (actual - predicted) ** 2\n",
    "\n",
    "def mse_grad(actual, predicted):\n",
    "    return predicted - actual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6532c821",
   "metadata": {},
   "source": [
    "THE BACKWAWRDS PASS\n",
    "- backpropagation to find weights and bias\n",
    "    - use chain rule to calc how much each weight contributed to erroer\n",
    "- gradient descent\n",
    "    - mudhe weights in the direction that reduces loss\n",
    "- calc the partial derivative of the loss with respect to each parmeter in the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05576c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsensor import explain as exp\n",
    "\n",
    "# compute the gradient of the mean squared error loss with respect to the model output\n",
    "output_gradient = mse_grad(tmax_tomorrow, output)\n",
    "\n",
    "with exp():\n",
    "    # compute the gradient of the loss with respect to the weights of layer 2 (l1_w)\n",
    "    # by multiplying the transposed activations from layer 1 with the output gradient\n",
    "    # tweak weights to reduce error\n",
    "    l2_w_gradient = l1_activated.T @ output_gradient\n",
    "l2_w_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dfa5a0",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L}{\\partial W_{2}}$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_{2}} = \\partial L\\frac{\\partial (XW_{2})}{\\partial W_{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a720102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import diff, symbols\n",
    "\n",
    "# show that it simpliies into just X\n",
    "x, w = symbols('X, W')\n",
    "# output of layer one * layer 2 weights\n",
    "sympy_output = x * w\n",
    "# differentiate it\n",
    "diff(sympy_output, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c4650",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L}{\\partial W_{2}} = \\partial LX$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8762548",
   "metadata": {},
   "outputs": [],
   "source": [
    "with exp():\n",
    "    l2_b_gradient = np.mean(output_gradient, axis=0)\n",
    "l2_b_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gradient descent to update weights and biases\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-5\n",
    "\n",
    "with exp():\n",
    "    l2_bias = l2_bias - l2_b_gradient * lr\n",
    "    l2_weights = l2_weights - l2_w_gradient * lr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cf1b7e",
   "metadata": {},
   "source": [
    "LAYER 1 GRADIENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c7aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with exp():\n",
    "    l1_activated_gradient = output_gradient @ l2_weights.T\n",
    "l1_activated_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1724b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = np.arange(-50,50)\n",
    "\n",
    "plt.plot(temps, np.maximum(0, temps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01254e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with exp():\n",
    "    l1_output_gradient = l1_activated_gradient * np.heaviside(l1_output, 0)\n",
    "l1_output_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec227d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation\n",
    "l1_w_gradient = input.T @ l1_output_gradient\n",
    "l1_b_gradient = np.mean(l1_output_gradient, axis=0)\n",
    "\n",
    "# gradient descent\n",
    "l1_weights -= l1_w_gradient * lr\n",
    "l1_bias  -= l1_b_gradient * lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f426f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
