{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e5906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statistics import mean\n",
    "\n",
    "# read the CSV file \"clean_weather.csv\" into a DataFrame.\n",
    "# set the first column (usually the date or unnamed index) as the DataFrame index.\n",
    "data = pd.read_csv(\"clean_weather.csv\", index_col=0)\n",
    "\n",
    "# forward-fill missing values (NaNs) in the DataFrame using the last known non-missing value.\n",
    "# this is useful in time series data where missing values can be inferred from prior entries.\n",
    "data = data.ffill()\n",
    "\n",
    "# create a scatter plot to visualize the relationship between today's maximum temperature (tmax)\n",
    "# and tomorrow's maximum temperature (tmax_tomorrow). This helps assess if a linear trend exists.\n",
    "data.plot.scatter(\"tmax\", \"tmax_tomorrow\")\n",
    "\n",
    "PREDICTORS = ['tmax', 'tmin', 'rain']\n",
    "TARGET = 'tmax_tomorrow'\n",
    "\n",
    "# all values are scaled so mean is 0\n",
    "scaler = StandardScaler()\n",
    "data[PREDICTORS] = scaler.fit_transform(data[PREDICTORS])\n",
    "\n",
    "split_data = np.split(data, [int(0.7*len(data)), int(0.85 * len(data))])\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = [[d[PREDICTORS].to_numpy(), d[[TARGET]].to_numpy()] for d in split_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99048ad",
   "metadata": {},
   "source": [
    "### NEURAL NETWORK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed62b927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_layers(inputs):\n",
    "    layers = []\n",
    "    for i in range(1, len(inputs)):\n",
    "        layers.append([\n",
    "            np.random.rand(inputs[i-1], inputs[i]) / 5 - .1,\n",
    "            np.ones((1, inputs[i]))\n",
    "        ])\n",
    "    return layers\n",
    "\n",
    "# second layer calculates 10 features from 3, etc, and output one\n",
    "layer_conf = [3, 10, 10, 1]\n",
    "layers = init_layers(layer_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f039a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76c619a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(batch, layers): \n",
    "    hiddens = [batch.copy()]\n",
    "    for i in range(len(layers)):\n",
    "        # batch = xw + b\n",
    "        batch = np.matmul(batch, layers[i][0]) + layers[i][1]\n",
    "        # dont apply relu on last layer\n",
    "        if i < len(layers) - 1:\n",
    "            batch = np.maximum(batch, 0)\n",
    "        hiddens.append(batch.copy())\n",
    "    return batch, hiddens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50b56509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate mean squared error\n",
    "def mse(actual, predicted):\n",
    "    # subtract the predicted values from the actual values, square the differences,\n",
    "    # then take the mean of all squared differences\n",
    "    return np.mean((actual - predicted) ** 2)\n",
    "\n",
    "def mse_grad(actual, predicted):\n",
    "    return predicted - actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7092004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(layers, hidden, grad, lr):\n",
    "    for i in range(len(layers)-1, -1, -1):\n",
    "        # if not at last layer, redo relu function\n",
    "        if i != len(layers) - 1:\n",
    "            grad = np.multiply(grad, np.heaviside(hidden[i+1], 0))\n",
    "\n",
    "        w_grad = hidden[i].T @ grad\n",
    "        b_grad = np.mean(grad, axis=0)\n",
    "\n",
    "        # update weights\n",
    "        layers[i][0] -= w_grad * lr\n",
    "        layers[i][1] -= b_grad * lr\n",
    "\n",
    "        grad = grad @ layers[i][0].T\n",
    "    return layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0159d3f9",
   "metadata": {},
   "source": [
    "### TRAINING ALGORITHM ###\n",
    "1. run network forward pass to get output\n",
    "2. compute gradient with respect to ouptus of the network (`mse_grad` function)\n",
    "3. for each layer in the network:\n",
    "    - compute the gradient with respect to the pre-nonlinearity ouptu (if the layer has a onlinearity)\n",
    "    - compute the gradient with respect to the weights\n",
    "    - compute the gradient with respect to the bias\n",
    "    - compute the gradient with respect to the inputs to the layer\n",
    "4. update the parameters in the network using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cadae1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train mse: 3271.3757601713846 valid mse: 1637.8230480271661\n",
      "Epoch 1 Train mse: 329.3480179087739 valid mse: 25.30680038124801\n",
      "Epoch 2 Train mse: 23.14049772043856 valid mse: 21.20815361500204\n",
      "Epoch 3 Train mse: 22.40587809288311 valid mse: 20.83838018289899\n",
      "Epoch 4 Train mse: 22.19142004967595 valid mse: 20.7638961110108\n",
      "Epoch 5 Train mse: 22.135737447589424 valid mse: 20.752861559180054\n",
      "Epoch 6 Train mse: 22.120027447648134 valid mse: 20.75354513580366\n",
      "Epoch 7 Train mse: 22.11427023909017 valid mse: 20.754785637414805\n",
      "Epoch 8 Train mse: 22.111023526710117 valid mse: 20.755464481973153\n",
      "Epoch 9 Train mse: 22.108607956121137 valid mse: 20.75555734541381\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-6\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "\n",
    "layers = init_layers(layer_conf)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = []\n",
    "\n",
    "    for i in range(0, train_x.shape[0], batch_size):\n",
    "        x_batch = train_x[i:(i+batch_size)]\n",
    "        y_batch = train_y[i:(i+batch_size)]\n",
    "\n",
    "        pred, hidden = forward(x_batch, layers)\n",
    "\n",
    "        loss = mse_grad(y_batch, pred)\n",
    "        epoch_loss.append(np.mean(loss**2))\n",
    "\n",
    "        layers = backward(layers, hidden, loss, lr)\n",
    "    \n",
    "    valid_preds, _ = forward(valid_x, layers)\n",
    "    print(f\"Epoch {epoch} Train mse: {mean(epoch_loss)} valid mse: {np.mean(mse(valid_preds, valid_y))}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
